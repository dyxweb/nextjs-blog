## 大文件切片上传
1. 使用Blob.prototype.slice方法将大文件分割成指定大小文件切片。
2. 使用formData并行上传所有文件切片数据（由于并行上传，将文件切片的顺序告诉服务端）。
3. 所有切片全部上传后通知服务端进行文件合并。
4. 服务端使用multiparty处理前端传的formData数据，存储所有文件切片数据。
5. 服务端根据文件切片的顺序使用读写流进行文件合并，合并成功后删除文件切片数据。
### 前端代码
```
import React, { useState } from 'react';
import { Button, message } from 'antd';
import request from '@/utils/request';

const SIZE = 10 * 1024 * 1024; // 单个文件切片大小
const BigFileUpload = () => {
  const [uploadStatus, setUploadStatus] = useState(''); // 上传的状态
  const [selectFile, setSelectFile] = useState<any>(null); // 选择的文件

  // 选择上传文件时存储文件信息
  const onUploadChange = (e: any) => {
    const [file] = e.target.files;
    if (file) {
      setSelectFile(file);
    }
  };

  // 生成文件切片
  const createFileChunk = (file: any) => {
    const fileChunkList = [];
    let cur = 0;
    while (cur < file.size) {
      fileChunkList.push({ file: file.slice(cur, cur + SIZE) });
      cur += SIZE;
    }
    return fileChunkList;
  };

  // 上传文件
  const uploadFile = () => {
    if (selectFile) {
      setUploadStatus('loading');
      const fileChunkList = createFileChunk(selectFile);
      const fileChunkData: any = fileChunkList.map(({ file }, index) => ({
        index,
        chunk: file,
        size: file.size
      }));
      uploadChunks(fileChunkData);
    }
  };

  // 上传切片
  const uploadChunks = async (fileChunkData: any) => {
    const requestList = fileChunkData
      .map(({ chunk, index }: { chunk: any; index: number }) => {
        const formData = new FormData();
        formData.append('chunk', chunk);
        formData.append('filename', selectFile.name);
        formData.append('index', String(index));
        return { formData, index };
      })
      .map(({ formData }: { formData: any; index: number }) =>
        request({
          url: '/upload',
          method: 'POST',
          data: formData
        })
      );
    await Promise.all(requestList);
    await mergeFile();
  };

  // 通知服务端合并切片
  const mergeFile = async () => {
    await request({
      url: '/merge',
      method: 'POST',
      data: JSON.stringify({
        size: SIZE,
        filename: selectFile.name
      })
    });
    message.success('上传成功');
    setUploadStatus('');
  };

  return (
    <div>
      {/* 正在上传中时disabled */}
      <input
        type="file"
        onChange={onUploadChange}
        disabled={uploadStatus === 'loading'}
      />
      {/* 没有选择文件或正在上传中时disabled */}
      <Button
        onClick={uploadFile}
        disabled={!selectFile || uploadStatus === 'loading'}
      >
        上传
      </Button>
    </div>
  );
};

export default BigFileUpload;
```
### 服务端代码
```
const multiparty = require("multiparty");
const fse = require("fs-extra");
const path = require("path");

// 大文件存储目录
const UPLOAD_DIR = path.resolve(__dirname, "..", "files");

// 写入文件流
const pipeStream = (path, writeStream) =>
  new Promise(resolve => {
    const readStream = fse.createReadStream(path);
    readStream.on("end", () => {
      fse.unlinkSync(path);
      resolve();
    });
    readStream.pipe(writeStream);
  });

// 提取POST请求参数
const resolvePost = req =>
  new Promise(resolve => {
    let chunk = "";
    req.on("data", data => {
      chunk += data;
    });
    req.on("end", () => {
      resolve(JSON.parse(chunk));
    });
  });

// 创建临时文件夹用于临时存储chunk (添加 chunkDir 前缀与文件名做区分)
const getChunkDir = fileName => path.resolve(UPLOAD_DIR, `chunkDir_${fileName}`);

// 合并切片
const mergeFileChunk = async (filePath, filename, size) => {
  const chunkDir = getChunkDir(filename);
  const chunkPaths = await fse.readdir(chunkDir);
  // 根据切片下标进行排序，否则直接读取目录的获得的顺序会错乱
  chunkPaths.sort((a, b) => a - b);

  // 并发写入文件
  await Promise.all(
    chunkPaths.map((chunkPath, index) =>
      pipeStream(
        path.resolve(chunkDir, chunkPath),
        // 根据 size 在指定位置创建可写流
        fse.createWriteStream(filePath, {
          start: index * size
        })
      )
    )
  );
  // 合并后删除保存切片的目录
  fse.rmdirSync(chunkDir);
};

module.exports = class {
  // 处理文件切片
  async handleFormData(req, res) {
    const multipart = new multiparty.Form();
    multipart.parse(req, async (err, fields, files) => {
      if (err) {
        console.error(err);
        res.status = 500;
        res.end(
          JSON.stringify({
            code: 100,
            message: "file error"
          })
        );
        return;
      }
      const [chunk] = files.chunk;
      const [filename] = fields.filename;
      const [index] = fields.index;
      const filePath = path.resolve(
        UPLOAD_DIR,
        `${filename}`
      ); // 最终合并后的文件路径
      const chunkDir = getChunkDir(filename); // 存放chunk的文件夹路径
      const chunkPath = path.resolve(chunkDir, index); // 存放每个切片文件的路径

      // 最终合并后的文件已经存在直接返回
      if (fse.existsSync(filePath)) {
        res.end(
          JSON.stringify({
            code: 0,
            message: "file exist"
          })
        );
        return;
      }

      // 切片存在直接返回
      if (fse.existsSync(chunkPath)) {
        res.end(
          JSON.stringify({
            code: 0,
            message: "chunk exist"
          })
        );
        return;
      }

      // 切片目录不存在，创建切片目录
      if (!fse.existsSync(chunkDir)) {
        await fse.mkdirs(chunkDir);
      }

      await fse.move(chunk.path, chunkPath);
      res.end(
        JSON.stringify({
          code: 0,
          message: "success"
        })
      );
    });
  }

  // 合并切片
  async handleMerge(req, res) {
    const data = await resolvePost(req);
    const { filename, size } = data;
    const filePath = path.resolve(UPLOAD_DIR, `${filename}`);
    await mergeFileChunk(filePath, filename, size);
    res.end(
      JSON.stringify({
        code: 0,
        message: "success"
      })
    );
  }
};
```
### 显示上传进度条
- 使用axios的onUploadProgress方法获取到切片上传的进度，可以独立显示每个文件切片的上传进度。
- 根据每个切片的上传进度计算出整个文件的上传进度可以显示整个文件的上传进度。